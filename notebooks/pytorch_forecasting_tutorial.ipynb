{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ecfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TimeSeriesDataSet, DeepAR, Baseline\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import QuantileLoss, RMSE, MAE, MAPE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "# Configure plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83863136",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing Data\n",
    "\n",
    "We'll load our EURUSD data and macroeconomic features, then prepare them for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature data\n",
    "features = pd.read_csv(\"../src/features/macro_features.csv\", parse_dates=[\"Date\"])\n",
    "features.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Load price data\n",
    "price_data = pd.read_csv(\"../src/data_fetch/EURUSD.csv\")\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "price_data.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Extract price and create labels for up/down movements\n",
    "price = price_data[\"EURUSD_Close\"]\n",
    "price_returns = price.pct_change().dropna()\n",
    "labels = price_returns.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Merge data\n",
    "combined_data = features.join(price.rename(\"EURUSD_Close\"), how=\"inner\")\n",
    "combined_data = combined_data.join(labels.rename(\"Label\"), how=\"inner\")\n",
    "combined_data.reset_index(inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199e03b",
   "metadata": {},
   "source": [
    "### 2.1 Visualize the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aedb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(combined_data['Date'], combined_data['EURUSD_Close'])\n",
    "plt.title('EURUSD Exchange Rate')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95861fee",
   "metadata": {},
   "source": [
    "## 3. Data Normalization\n",
    "\n",
    "Neural networks work best with normalized data. We'll normalize both features and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "feature_columns = [col for col in combined_data.columns \n",
    "                   if col not in ['Date', 'Label', 'EURUSD_Close']]\n",
    "\n",
    "# Initialize scalers\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "train_size = int(len(combined_data) * 0.8)\n",
    "train_data = combined_data.iloc[:train_size].copy()\n",
    "test_data = combined_data.iloc[train_size:].copy()\n",
    "\n",
    "# Fit and transform feature scaler on training data\n",
    "train_features = train_data[feature_columns]\n",
    "normalized_train_features = feature_scaler.fit_transform(train_features)\n",
    "\n",
    "# Transform test features\n",
    "test_features = test_data[feature_columns]\n",
    "normalized_test_features = feature_scaler.transform(test_features)\n",
    "\n",
    "# Update dataframes with normalized features\n",
    "for i, col in enumerate(feature_columns):\n",
    "    train_data[col] = normalized_train_features[:, i]\n",
    "    test_data[col] = normalized_test_features[:, i]\n",
    "\n",
    "# Normalize target (EURUSD_Close)\n",
    "train_target = train_data['EURUSD_Close'].values.reshape(-1, 1)\n",
    "train_data['EURUSD_Close_normalized'] = target_scaler.fit_transform(train_target).flatten()\n",
    "\n",
    "test_target = test_data['EURUSD_Close'].values.reshape(-1, 1)\n",
    "test_data['EURUSD_Close_normalized'] = target_scaler.transform(test_target).flatten()\n",
    "\n",
    "# Display normalized data\n",
    "train_data[['Date', 'EURUSD_Close', 'EURUSD_Close_normalized'] + feature_columns[:3]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e578444",
   "metadata": {},
   "source": [
    "## 4. Preparing TimeSeriesDataSet Format\n",
    "\n",
    "PyTorch Forecasting requires data in a specific format. We need to prepare our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add required columns for TimeSeriesDataSet\n",
    "train_data['time_idx'] = range(len(train_data))  # Time index starting from 0\n",
    "train_data['group_id'] = 'forex'  # All rows belong to the same group\n",
    "\n",
    "test_data['time_idx'] = range(len(train_data), len(train_data) + len(test_data))\n",
    "test_data['group_id'] = 'forex'\n",
    "\n",
    "# Define forecasting parameters\n",
    "max_encoder_length = 30  # Use 30 days of history\n",
    "prediction_length = 1    # Predict 1 day ahead\n",
    "\n",
    "# Create training dataset\n",
    "training_data = TimeSeriesDataSet(\n",
    "    data=train_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"EURUSD_Close_normalized\",\n",
    "    group_ids=[\"group_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=prediction_length,\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=feature_columns,  # Our macro features are known\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\"EURUSD_Close_normalized\"],  # Target is unknown in future\n",
    "    target_normalizer=None,  # We manually normalized the target\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=False,\n",
    "    randomize_length=None,\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "validation_data = TimeSeriesDataSet.from_dataset(\n",
    "    training_data, \n",
    "    test_data, \n",
    "    predict=True, \n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataloader = training_data.to_dataloader(\n",
    "    batch_size=batch_size, \n",
    "    num_workers=0, \n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = validation_data.to_dataloader(\n",
    "    batch_size=batch_size, \n",
    "    num_workers=0, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5d3d3",
   "metadata": {},
   "source": [
    "## 5. Building and Training the DeepAR Model\n",
    "\n",
    "Now we'll create and train the DeepAR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89846ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DeepAR model\n",
    "model = DeepAR.from_dataset(\n",
    "    training_data,\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=64,\n",
    "    rnn_layers=2,\n",
    "    dropout=0.3,\n",
    "    loss=QuantileLoss(),\n",
    ")\n",
    "\n",
    "# Configure trainer with early stopping\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=1e-4, \n",
    "    patience=10, \n",
    "    verbose=True, \n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto',  # Use GPU if available\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b432a",
   "metadata": {},
   "source": [
    "## 6. Making Predictions\n",
    "\n",
    "Let's use our trained model to make predictions and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation data\n",
    "predictions = model.predict(val_dataloader)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "pred_median = predictions.prediction[:, -1, 1].cpu().numpy()  # Median (0.5 quantile)\n",
    "pred_lower = predictions.prediction[:, -1, 0].cpu().numpy()   # Lower (0.1 quantile)\n",
    "pred_upper = predictions.prediction[:, -1, 2].cpu().numpy()   # Upper (0.9 quantile)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "pred_median_orig = target_scaler.inverse_transform(pred_median.reshape(-1, 1)).flatten()\n",
    "pred_lower_orig = target_scaler.inverse_transform(pred_lower.reshape(-1, 1)).flatten()\n",
    "pred_upper_orig = target_scaler.inverse_transform(pred_upper.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Get actual values\n",
    "actual_values = test_data['EURUSD_Close'].values\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': test_data['Date'],\n",
    "    'Actual': actual_values,\n",
    "    'Predicted': pred_median_orig,\n",
    "    'Lower_CI': pred_lower_orig,\n",
    "    'Upper_CI': pred_upper_orig\n",
    "})\n",
    "\n",
    "# Show the first few predictions\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d692c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(results_df['Date'], results_df['Actual'], label='Actual', color='blue')\n",
    "plt.plot(results_df['Date'], results_df['Predicted'], label='Predicted', color='red', linestyle='--')\n",
    "plt.fill_between(results_df['Date'], results_df['Lower_CI'], results_df['Upper_CI'], \n",
    "                 color='red', alpha=0.2, label='90% Prediction Interval')\n",
    "plt.title('DeepAR Predictions vs Actual EURUSD Exchange Rate')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa17451",
   "metadata": {},
   "source": [
    "## 7. Generating Trading Signals\n",
    "\n",
    "Now let's generate trading signals based on our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trading signals based on predicted direction\n",
    "# 1 for predicted price increase, 0 for predicted decrease\n",
    "results_df['Signal'] = (results_df['Predicted'] > results_df['Actual'].shift(1)).astype(int)\n",
    "\n",
    "# Calculate the confidence based on prediction interval width\n",
    "results_df['Interval_Width'] = results_df['Upper_CI'] - results_df['Lower_CI']\n",
    "results_df['Confidence'] = 1 - (results_df['Interval_Width'] / results_df['Actual'])\n",
    "\n",
    "# Calculate actual returns\n",
    "results_df['Actual_Return'] = results_df['Actual'].pct_change()\n",
    "\n",
    "# Calculate strategy returns\n",
    "# For signal=1 (buy), we get the next day's return\n",
    "# For signal=0 (sell), we get the negative of the next day's return\n",
    "results_df['Strategy_Return'] = np.where(\n",
    "    results_df['Signal'] == 1,\n",
    "    results_df['Actual_Return'].shift(-1),  # Next day's return for buy signals\n",
    "    -results_df['Actual_Return'].shift(-1)  # Negative of next day's return for sell signals\n",
    ")\n",
    "\n",
    "# Calculate cumulative returns\n",
    "results_df['Cumulative_Market_Return'] = (1 + results_df['Actual_Return']).cumprod() - 1\n",
    "results_df['Cumulative_Strategy_Return'] = (1 + results_df['Strategy_Return'].fillna(0)).cumprod() - 1\n",
    "\n",
    "# Remove NaN values\n",
    "results_df = results_df.dropna()\n",
    "\n",
    "# Display the results\n",
    "results_df[['Date', 'Actual', 'Predicted', 'Signal', 'Confidence', \n",
    "            'Actual_Return', 'Strategy_Return']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns comparison\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(results_df['Date'], results_df['Cumulative_Market_Return'], \n",
    "         label='Market Returns (Buy & Hold)', color='blue')\n",
    "plt.plot(results_df['Date'], results_df['Cumulative_Strategy_Return'], \n",
    "         label='DeepAR Strategy Returns', color='green')\n",
    "plt.title('Cumulative Returns Comparison')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc01d5",
   "metadata": {},
   "source": [
    "## 8. Performance Evaluation\n",
    "\n",
    "Let's evaluate the performance of our trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "total_return = results_df['Cumulative_Strategy_Return'].iloc[-1]\n",
    "annual_return = (1 + total_return) ** (252 / len(results_df)) - 1  # Annualized return assuming 252 trading days\n",
    "sharpe_ratio = results_df['Strategy_Return'].mean() / results_df['Strategy_Return'].std() * np.sqrt(252)\n",
    "win_rate = len(results_df[results_df['Strategy_Return'] > 0]) / len(results_df)\n",
    "max_drawdown = (results_df['Cumulative_Strategy_Return'] / \n",
    "                results_df['Cumulative_Strategy_Return'].cummax() - 1).min()\n",
    "\n",
    "# Print performance summary\n",
    "print(f\"DeepAR Strategy Performance Summary:\")\n",
    "print(f\"Total Return: {total_return:.2%}\")\n",
    "print(f\"Annualized Return: {annual_return:.2%}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(f\"Maximum Drawdown: {max_drawdown:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2c184",
   "metadata": {},
   "source": [
    "## 9. Adjusting the Strategy for Different Market Conditions\n",
    "\n",
    "We can adjust our strategy based on prediction confidence and market volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7574e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling volatility over 30 days\n",
    "results_df['Rolling_Volatility'] = results_df['Actual_Return'].rolling(30).std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Create a more conservative strategy that only trades when confidence is high\n",
    "confidence_threshold = results_df['Confidence'].median()  # Use median confidence as threshold\n",
    "results_df['Conservative_Signal'] = np.where(\n",
    "    results_df['Confidence'] > confidence_threshold,\n",
    "    results_df['Signal'],  # Use original signal if confidence is high\n",
    "    np.nan  # No trade if confidence is low\n",
    ")\n",
    "\n",
    "# Calculate conservative strategy returns\n",
    "results_df['Conservative_Return'] = np.where(\n",
    "    results_df['Conservative_Signal'] == 1,\n",
    "    results_df['Actual_Return'].shift(-1),  # Next day's return for buy signals\n",
    "    np.where(\n",
    "        results_df['Conservative_Signal'] == 0,\n",
    "        -results_df['Actual_Return'].shift(-1),  # Negative of next day's return for sell signals\n",
    "        0  # No return when not trading\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate cumulative returns for conservative strategy\n",
    "results_df['Cumulative_Conservative_Return'] = (1 + results_df['Conservative_Return'].fillna(0)).cumprod() - 1\n",
    "\n",
    "# Calculate number of trades for each strategy\n",
    "original_trades = results_df['Signal'].diff().abs().sum()\n",
    "conservative_trades = results_df['Conservative_Signal'].dropna().diff().abs().sum()\n",
    "\n",
    "print(f\"Original Strategy Trades: {original_trades}\")\n",
    "print(f\"Conservative Strategy Trades: {conservative_trades}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all strategies together\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(results_df['Date'], results_df['Cumulative_Market_Return'], \n",
    "         label='Market (Buy & Hold)', color='blue')\n",
    "plt.plot(results_df['Date'], results_df['Cumulative_Strategy_Return'], \n",
    "         label='DeepAR Strategy', color='green')\n",
    "plt.plot(results_df['Date'], results_df['Cumulative_Conservative_Return'], \n",
    "         label='Conservative DeepAR', color='purple')\n",
    "plt.title('Cumulative Returns Comparison')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a99bc1",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis\n",
    "\n",
    "Let's try to understand which features are most important for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate feature importance through permutation\n",
    "def calculate_permutation_importance(model, dataloader, feature_names, n_repeats=5):\n",
    "    \"\"\"Calculate permutation importance for each feature\"\"\"\n",
    "    # Get baseline loss\n",
    "    baseline_predictions = model.predict(dataloader)\n",
    "    baseline_loss = model.loss(baseline_predictions.output, baseline_predictions.y)\n",
    "    baseline_loss = baseline_loss.item()\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for feature_name in feature_names:\n",
    "        feature_importances = []\n",
    "        \n",
    "        for _ in range(n_repeats):\n",
    "            # Create a copy of the dataloader with permuted feature\n",
    "            permuted_dataloader = dataloader.dataset.copy_dataset()\n",
    "            \n",
    "            # Permute the feature\n",
    "            for batch in permuted_dataloader:\n",
    "                if feature_name in batch.x_cat_names:\n",
    "                    idx = batch.x_cat_names.index(feature_name)\n",
    "                    batch.x_cat[:, :, idx] = batch.x_cat[:, :, idx][torch.randperm(batch.x_cat.shape[0])]\n",
    "                elif feature_name in batch.x_cont_names:\n",
    "                    idx = batch.x_cont_names.index(feature_name)\n",
    "                    batch.x_cont[:, :, idx] = batch.x_cont[:, :, idx][torch.randperm(batch.x_cont.shape[0])]\n",
    "            \n",
    "            # Get predictions and loss with permuted feature\n",
    "            permuted_predictions = model.predict(permuted_dataloader)\n",
    "            permuted_loss = model.loss(permuted_predictions.output, permuted_predictions.y)\n",
    "            permuted_loss = permuted_loss.item()\n",
    "            \n",
    "            # Calculate importance\n",
    "            feature_importance = permuted_loss - baseline_loss\n",
    "            feature_importances.append(feature_importance)\n",
    "        \n",
    "        # Calculate mean importance across repeats\n",
    "        importances.append(np.mean(feature_importances))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Try to get feature importance\n",
    "try:\n",
    "    feature_importance = calculate_permutation_importance(\n",
    "        model, val_dataloader, feature_columns, n_repeats=3\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "    plt.title('Top 15 Features by Permutation Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate permutation importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66fb391",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've learned how to:\n",
    "\n",
    "1. Prepare time series data for PyTorch Forecasting\n",
    "2. Build and train a DeepAR model for forecasting\n",
    "3. Generate trading signals based on the predictions\n",
    "4. Evaluate the strategy's performance\n",
    "5. Create a more conservative version of the strategy\n",
    "\n",
    "### Potential next steps:\n",
    "\n",
    "1. **Hyperparameter Optimization**: Use techniques like Optuna to find optimal parameters\n",
    "2. **Ensemble Methods**: Combine DeepAR with other models for better predictions\n",
    "3. **Feature Engineering**: Create more advanced features that capture market regimes\n",
    "4. **Risk Management**: Implement dynamic position sizing based on prediction confidence\n",
    "5. **Multi-horizon Forecasting**: Predict at multiple time horizons (e.g., 1-day, 3-day, and 5-day ahead)\n",
    "6. **Walk-forward Validation**: Implement continuous retraining on rolling windows of data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
